import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import os
from logging_config import setup_logger, log_execution_time
from error_handler import error_handler

# ë¡œê±° ì„¤ì •
logger = setup_logger('news_scraper')
start_time = datetime.now()

keywords = {
    'ë¶€ë™ì‚°': ['ì„œìš¸', 'ì•„íŒŒíŠ¸', 'ë¶€ë™ì‚°', 'ì „ì„¸', 'ì¬ê±´ì¶•', 'ì…ì£¼', 'ì‹¤ê±°ë˜', 'ì²­ì•½', 'ë¶„ì–‘', 'ë§¤ë§¤', 'ê±°ë˜ëŸ‰', 'ì¤‘ê°œì—…ì†Œ'],
    'ê¸ˆë¦¬': ['ê¸ˆë¦¬', 'ì—°ì¤€', 'ì¸ìƒ', 'ì¸í•˜', 'ê¸°ì¤€ê¸ˆë¦¬', 'ë¬¼ê°€', 'CPI', 'ë¬¼ê°€ìƒìŠ¹ë¥ ', 'ê¸ˆí†µìœ„', 'ì±„ê¶Œ', 'ìœ ë™ì„±'],
    'í•´ì™¸ì£¼ì‹': ['ë‚˜ìŠ¤ë‹¥', 'S&P', 'í…ŒìŠ¬ë¼', 'ì• í”Œ', 'ì—”ë¹„ë””ì•„', 'ë¹„íŠ¸ì½”ì¸', 'ETF', 'ë‰´ìš•ì¦ì‹œ', 'AIì£¼', 'ë°˜ë„ì²´', 'ë¯¸êµ­ì£¼ì‹']
}

def extract_first_paragraph(url):
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.text, 'html.parser')
        content = soup.select_one("#dic_area")
        if content:
            paragraphs = content.get_text(strip=True).split('\n')
            for p in paragraphs:
                if len(p.strip()) > 30:
                    return p.strip()
        return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
    except Exception as e:
        logger.warning(f"URL {url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return "ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨"

logger.info("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

@error_handler('news_scraper', notify_success=True)  # ì„±ê³µ ì•Œë¦¼ ë°›ê¸°
def main():
    url = "https://news.naver.com/main/ranking/popularDay.naver?mid=etc&sid1=101"
    res = requests.get(url)
    soup = BeautifulSoup(res.text, 'html.parser')
    articles = soup.select('.rankingnews_box a')
    
    logger.info(f"ì´ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
    
    results = {k: [] for k in keywords}
    for article in articles:
        title = article.text.strip()
        href = article['href']
        if not href.startswith("http"):
            link = "https://news.naver.com" + href
        else:
            link = href
    
        for category, words in keywords.items():
            if any(word in title for word in words):
                if len(results[category]) < 10:
                    paragraph = extract_first_paragraph(link)
                    results[category].append((title, link, paragraph))
                break
    
    # ë‚ ì§œë³„ í´ë” ìƒì„±
    today = (datetime.now() + timedelta(hours=9)).strftime('%Y-%m-%d')
    year_month = (datetime.now() + timedelta(hours=9)).strftime('%Y/%m')
    output_dir = f"data/raw/{year_month}"
    os.makedirs(output_dir, exist_ok=True)
    
    # íŒŒì¼ ì €ì¥
    output_file = f"{output_dir}/output_{today}.md"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"# ğŸ“… {today} ë„¤ì´ë²„ ê²½ì œ í‚¤ì›Œë“œ ë‰´ìŠ¤ ìš”ì•½\n\n")
        for cat, items in results.items():
            if len(items) >= 3:
                logger.info(f"{cat} ì¹´í…Œê³ ë¦¬: {len(items)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                f.write(f"## ğŸ“Œ {cat}\n\n")
                for i, (title, link, para) in enumerate(items[:10], 1):
                    f.write(f"{i}. **{title}**\n   - {para}\n   - [ê¸°ì‚¬ ë§í¬]({link})\n\n")
    
    logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
    log_execution_time(logger, start_time, 'news_scraper')

if __name__ == "__main__":
    main()
